---
title: "Matrix Efficiencies for MCMC"
date: "May 20, 2020 "
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction
Indicative report comparing R matrix efficiencies efficiencies. The initial investigation is on R native solutions including techniques and support libraries for efficiencies. Elementwise function change to 2 dimiensional matrix of 1000 x 1000 matrix with:


* stardard looping through matrix
* sapply
* parrallel (n clusters)


*To measure perfomance proc.time function was applied to determines how much real and CPU time (in seconds) the currently running R process has already taken.
The user time is the CPU time charged for the execution of user instructions of the calling process. The system time is the CPU time charged for execution by the system on behalf of the calling process.* [1](https://stat.ethz.ch/R-manual/R-devel/library/base/html/proc.time.html)

```{r echo=FALSE, message=FALSE, warning=FALSE}
#summary(pressure)
matris1=matrix(rnorm(10),1000,1000)
# loop through each value and add 10:
#with for loop
mat = matris1
ptm <- proc.time()
# Create the loop with r and c to iterate over the matrix
for (r in 1:nrow(mat))   
  for (c in 1:ncol(mat))  
    mat[r,c] = mat[r,c] * 10
R_matrix_for = proc.time() - ptm

# sapply Multiply all values by 10:
#with sapply
ptm <- proc.time()
sapply_matrix = sapply(matris1,function(x) 10 * x)
R_matrix_sapply = proc.time() - ptm
#-----------------------------------

# parallelize computations using a cluster.
# equivalet with parrellel sapply, Multiply all values by 10:
library("parallel")
#------------2cores-----------------------
cl = makeCluster(2) #set number of cluster objects
ptm <- proc.time()
parSapply_parrallel_matrix = parSapply(cl,matris1,function(x) 10 * x)

stopCluster(cl) #always stop cluter, avoid memory leak
parSapply2_parrallel_matrix_time = proc.time() - ptm

#------------3cores-----------------------
cl = makeCluster(3) #set number of cluster objects
ptm <- proc.time()
parSapply_parrallel_matrix = parSapply(cl,matris1,function(x) 10 * x)

stopCluster(cl) #always stop cluter, avoid memory leak
parSapply3_parrallel_matrix_time = proc.time() - ptm

#------------4cores-----------------------
cl = makeCluster(4) #set number of cluster objects
ptm <- proc.time()
parSapply_parrallel_matrix = parSapply(cl,matris1,function(x) 10 * x)

stopCluster(cl) #always stop cluter, avoid memory leak
parSapply4_parrallel_matrix_time = proc.time() - ptm


results_survey <- data.frame("for_loop" = c(R_matrix_for),
                     "sapply" = c(R_matrix_sapply),
                     "parallel_2" = c(parSapply2_parrallel_matrix_time),
                     "parallel_3" = c(parSapply3_parrallel_matrix_time),
                     "parallel_4" = c(parSapply4_parrallel_matrix_time)
                     )

results_survey<- head(results_survey,-2)#remove user.child and sys.child recored

#three decimal places
is.num <- sapply(results_survey, is.numeric)
results_survey[is.num] <- lapply(results_survey[is.num], round, 3)
library(gridExtra)
#table output
grid.table(results_survey)

```

## Results
sapply performce show the lowest elapse time on a 1000 x 1000  matrix, this is likely due to the overhead of running jobs in parallel.
If the jobs you fire at the worker nodes take a significant amount of time then it is likely parallelization will improve overall performance. If individual jobs take only milliseconds, the overhead of constantly firing off jobs will deteriorate overall performance. Further optimising is possible by dividing the work over the nodes in such a way that the jobs are sufficiently long, say at least a few second. Parrallelisation advantages are shown when runing models simultaneously that individual will run for hours.


## Run Time Plots

Computational plots:

```{r pressure, echo=FALSE}
#plot(pressure)
#pressure
```


## Possible Improvements to R code 
If R code isn't fast enough due to bottle necks the following mabye possible.

1. Rewrite functions in C/C++, the bottlenecks that can be address are:

* Loops that cant be easily vectorised because subsequent iterations depend on previous ones.
* Recursive functions
* Problems that require advanced data structures

2. Frameworks 

* tensorflow (utilisig monder architecturd -GPU)
* mapreduce, Hadoop (clusters for big data)

3. Other improved library languagues

* GNU GSL
* Stan
